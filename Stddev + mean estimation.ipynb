{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.denoising_utils import *\n",
    "\n",
    "FOLDER = \"BSDS300/images/train/\"\n",
    "IMAGES = [FOLDER + img for img in os.listdir(FOLDER)][:100]\n",
    "def L2(A):\n",
    "    A_1D = np.reshape(A, (-1))\n",
    "    return np.sqrt(A_1D.dot(A_1D) / A_1D.shape[0])\n",
    "    \n",
    "data = []\n",
    "for sigma in np.arange(0,105,5):\n",
    "    print(\"{}/100\".format(sigma))\n",
    "    stddev = []\n",
    "    mean = []\n",
    "    for fname in IMAGES:\n",
    "        img_pil = crop_image(get_image(fname, -1)[0], d=32)\n",
    "        img_np = pil_to_np(img_pil)\n",
    "        _, img_noisy_np = get_noisy_image(img_np, sigma/255)\n",
    "        diff_n = img_noisy_np - img_np\n",
    "        \n",
    "        stddev.append(diff_n.std()*255)\n",
    "        mean.append(diff_n.mean()*255)\n",
    "    data.append((sigma, np.mean(stddev), np.mean(mean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, = plt.plot([(x[0]) for x in data], [(x[0]) for x in data], label=\"Theoretical unclipped method noise \", linestyle=\"dotted\")\n",
    "b, = plt.plot([(x[0]) for x in data], [(x[1]) for x in data], label=\"Empirical clipped method noise\")\n",
    "plt.legend(handles=[a, b])\n",
    "plt.ylabel(\"Measured standard deviation of the method noise\")\n",
    "plt.xlabel(\"Standard deviation Ïƒ of the corrupting noise N\")\n",
    "axes = plt.gca()\n",
    "# axes.set_xlim([xmin,xmax])\n",
    "# axes.set_ylim([0,100])\n",
    "plt.savefig(fname=\"method_noise_vs_corrupting_noise.pdf\", format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_se = [-3.1389654e-6, -7.0210049e-1, -1.8598057, 3.9881199, -8.3760888, 9.7330082, -6.9596693, 2.9464712, -7.3358565e-1, 9.9630886e-2, -5.7155088e-3]\n",
    "coeffs_sm = [1.6923658e-1, 7.0309281e-1, 3.7234715e-2, 2.4377832e-2, 2.0282884e-3, -1.7851033e-5, -8.5123452e-5, -2.6295693e-5, 3.2172868e-7, 1.4308530e-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype = torch.cuda.FloatTensor\n",
    "# p = np.polynomial.Polynomial(coeffs)\n",
    "def p_se(x):\n",
    "    return sum([(x**p)*coeff for p, coeff in enumerate(coeffs_se)])\n",
    "\n",
    "def p_sm(x):\n",
    "    return sum([(x**p)*coeff for p, coeff in enumerate(coeffs_sm)])\n",
    "\n",
    "def Se(ksi):\n",
    "    if not torch.is_tensor(ksi):\n",
    "        ksi = dtype([ksi])\n",
    "    return 1 - torch.exp(p_se(torch.sqrt(ksi)))\n",
    "\n",
    "def Sm(mu):\n",
    "    if not torch.is_tensor(mu):\n",
    "        mu = dtype([mu])\n",
    "    return (1+torch.tanh(p_sm(mu)))/2\n",
    "\n",
    "def clipped_var(image, variance):\n",
    "    return variance*Se(image/variance)*Se((1-image)/variance)# + 0.5*variance*Se(image/variance)*Se((1-image)/variance)\n",
    "\n",
    "def rms(values):\n",
    "    return torch.sqrt((torch.sum(values*values))/values.nelement())\n",
    "\n",
    "def average_clipped_var(image, variance):\n",
    "    image = torch.from_numpy(image).type(dtype)\n",
    "#     return torch.sqrt(torch.sum(clipped_var(image, variance)**2)/image.nelement())\n",
    "    return torch.mean(clipped_var(image, variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_var(1.0, 100/255)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "sigma = 50\n",
    "v = np.full(n, 1.0)\n",
    "noise = np.random.normal(loc=np.full(n, 0), scale=sigma/255)\n",
    "clipped = np.clip(v + noise, 0.0, 1.0)\n",
    "# print(255*np.abs(-v + clipped))\n",
    "print(255*L2(-v + clipped))\n",
    "print(255*average_clipped_var(clipped, sigma/255))\n",
    "# print(clipped)\n",
    "# print(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = []\n",
    "mean = []\n",
    "sigma = 100\n",
    "for fname in IMAGES:\n",
    "    img_pil = crop_image(get_image(fname, -1)[0], d=32)\n",
    "    img_np = pil_to_np(img_pil)\n",
    "    _, img_noisy_np = get_noisy_image(img_np, sigma/255, clip=True)\n",
    "    diff_n = img_noisy_np - img_np\n",
    "\n",
    "    \n",
    "    computed = diff_n.std()*255\n",
    "#     expected = computed\n",
    "    expected = 255*average_clipped_var(img_noisy_np, sigma/255)\n",
    "    print(\"Computed:\\t{:.2f}\\tPredicted:\\t{:.2f}\\tDelta:  {:.2f}\\tvar:\\t{:.2f}\\tmean:\\t{:.2f}\".format(\n",
    "        computed, expected, computed-expected, img_noisy_np.std(), img_noisy_np.mean()\n",
    "        \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.full(200, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.arange(0,105,5)\n",
    "mus = np.linspace(0.0, 1.0, num=101)\n",
    "out = np.empty((sigmas.shape[0], mus.shape[0]), dtype=np.float)\n",
    "\n",
    "for i, sigma in enumerate(sigmas):\n",
    "    print(\"Sigma = {}\".format(sigma))\n",
    "    for j, mu in enumerate(mus):\n",
    "        n = 1000000\n",
    "        v = np.full(n, mu)\n",
    "        noise = np.random.normal(loc=np.full(n, 0), scale=sigma/255)\n",
    "        clipped = np.clip(v + noise, 0.0, 1.0)\n",
    "        out[i][j] = np.std(-v + clipped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sigma in enumerate(sigmas):\n",
    "    print(\"Sigma = {}\".format(sigma))\n",
    "    n = 10000000\n",
    "    v = np.random.uniform(size=n)\n",
    "    noise = np.random.normal(loc=np.full(n, 0), scale=sigma/255)\n",
    "    clipped = np.clip(v + noise, 0.0, 1.0)\n",
    "    for j in range(0, 255):\n",
    "        cur_clipped = clipped[np.logical_and(clipped < j+1/100, j/100 < clipped)]\n",
    "        cur_v = v[np.logical_and(clipped < j+1/100, j/100 < clipped)]\n",
    "        out[i][j] = np.std(-cur_v + cur_clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "255*out[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([255*out[x][0] for x in range(0,21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = [c*5 for c in range(out.shape[0])]\n",
    "y = range(out.shape[1])\n",
    "\n",
    "X, Y = np.meshgrid(y, x)\n",
    "ax.plot_surface(X=X, Y=Y, Z=out, rstride=1, cstride=1, cmap=plt.cm.ocean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = torch.from_numpy(out)\n",
    "# torch.save(table, \"sigma_mu_to_std.tensor\")\n",
    "if 'table' not in locals():\n",
    "    table = torch.load(\"sigma_mu_to_std.tensor\")\n",
    "g_table = table.cuda()\n",
    "\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def pred_std(image, sigma):\n",
    "    image = torch.from_numpy(image).type(dtype)\n",
    "    pred = 0.0\n",
    "    for channel in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            for y in range(image.shape[2]):\n",
    "                pred += g_table[int(sigma/5)][int(100*image[channel][x][y])]**2\n",
    "    return np.sqrt(pred/image.nelement())\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = []\n",
    "mean = []\n",
    "sigma = 100\n",
    "for fname in IMAGES:\n",
    "    img_pil = crop_image(get_image(fname, -1)[0], d=32)\n",
    "    img_np = pil_to_np(img_pil)\n",
    "    _, img_noisy_np = get_noisy_image(img_np, sigma/255)\n",
    "    diff_n = img_noisy_np - img_np\n",
    "    computed = diff_n.std()*255\n",
    "    expected = 255*pred_std(img_noisy_np, sigma)\n",
    "    print(\"Computed:\\t{:.2f}\\tPredicted:\\t{:.2f}\\tDelta:  {:.2f}\\tvar:\\t{:.2f}\\tmean:\\t{:.2f}\".format(\n",
    "        computed, expected, computed-expected, img_noisy_np.std(), img_noisy_np.mean()\n",
    "        \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.arange(0,105,5)\n",
    "mus = np.arange(0,256,1)\n",
    "out = np.empty((sigmas.shape[0], mus.shape[0]), dtype=np.float)\n",
    "\n",
    "for i, sigma in enumerate(sigmas):\n",
    "# for i, sigma in enumerate([50]):\n",
    "    print(\"Sigma = {}\".format(sigma))\n",
    "    n = 10000000\n",
    "    v = np.random.uniform(size=n)\n",
    "    noise = np.random.normal(loc=np.full(n, 0), scale=sigma/255)\n",
    "    unclipped = v+noise\n",
    "    clipped = np.clip(unclipped, 0.0, 1.0)\n",
    "#     plt.scatter(v, clipped)\n",
    "    v *= 256\n",
    "    v = v.astype(int, copy=False)\n",
    "    clipped *= 256\n",
    "    clipped = clipped.astype(int, copy=False)\n",
    "    D = np.abs(clipped - v)\n",
    "    \n",
    "    for j in range(0, 256):\n",
    "        out[i][j] = D[v == j].mean()\n",
    "plt.plot(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = torch.from_numpy(out)\n",
    "# torch.save(table, \"sigma_mu_to_std.tensor\")\n",
    "if 'table' not in locals():\n",
    "    table = torch.load(\"sigma_mu_to_std.tensor\")\n",
    "g_table = table.cuda()\n",
    "\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "def pred_std(image, sigma):\n",
    "    image = torch.from_numpy(image).type(dtype)\n",
    "    pred = 0.0\n",
    "    for channel in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            for y in range(image.shape[2]):\n",
    "                pred += (g_table[int(sigma/5)][int(255*image[channel][x][y])])**2\n",
    "    return np.sqrt(pred/image.nelement())\n",
    "\n",
    "plt.plot(out[20])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = []\n",
    "mean = []\n",
    "sigma = 100\n",
    "for fname in IMAGES:\n",
    "    img_pil = crop_image(get_image(fname, -1)[0], d=32)\n",
    "    img_np = pil_to_np(img_pil)\n",
    "    _, img_noisy_np = get_noisy_image(img_np, sigma/255)\n",
    "    diff_n = img_noisy_np - img_np\n",
    "    computed = diff_n.std()*255\n",
    "    expected = pred_std(img_noisy_np, sigma)\n",
    "    print(\"Computed:\\t{:.2f}\\tPredicted:\\t{:.2f}\\tDelta:  {:.2f}\\tvar:\\t{:.2f}\\tmean:\\t{:.2f}\".format(\n",
    "        computed, expected, computed-expected, img_noisy_np.std(), img_noisy_np.mean()\n",
    "        \n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
